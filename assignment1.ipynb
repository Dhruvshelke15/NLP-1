{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed452c-4232-4791-a6e5-e93520126ecb",
   "metadata": {},
   "source": [
    "### UnsMoothed Unigram & Bigram Language Models (MLE)\n",
    "\n",
    "1) Loads a tokenized corpus where each line is one review (space-delimited tokens).\n",
    "2) Builds unsmoothed unigram and bigram counts.\n",
    "3) Converts counts to MLE probabilities.\n",
    "4) Shows top-K previews as interactive tables.\n",
    "5) Saves counts & probabilities to TSV/JSON for downstream use.\n",
    "\n",
    "ðŸ“Œ Notes\n",
    "- This is PURE MLE (no smoothing, no UNK). We'll add smoothing & perplexity later.\n",
    "- For bigrams, using sentence boundary tokens (<s>, </s>) is helpful; toggle via ADD_BOUNDARIES below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfcd8b4-3762-4570-9333-c5f372724f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (25.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhruv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bce621-afb6-4d9d-986b-c8638563418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Iterable, Set\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d667bac1-9253-4225-8064-9678bd939769",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "def trans_lower(text) :\n",
    "    for i in text:\n",
    "        i = i.lower()\n",
    "    return text\n",
    "\n",
    "def read_corpus(path: str, add_boundaries: bool = False) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Reads a tokenized corpus; each line is a review with space-delimited tokens.\n",
    "    If add_boundaries=True, wraps each line with BOS/EOS.\n",
    "    Returns: List[List[str]] where each inner list is the token sequence for one line.\n",
    "    \"\"\"\n",
    "    sents: List[List[str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            toks = trans_lower(toks)\n",
    "            if add_boundaries:\n",
    "                toks = [BOS] + toks + [EOS]\n",
    "            sents.append(toks)\n",
    "    return sents\n",
    "\n",
    "def iter_corpus(path: str, add_boundaries: bool = False) -> Iterable[List[str]]:\n",
    "    \"\"\"\n",
    "    Memory-light generator version. Yields token lists per line.\n",
    "    Useful if your corpus is huge.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            toks = trans_lower(toks)\n",
    "            if add_boundaries:\n",
    "                toks = [BOS] + toks + [EOS]\n",
    "            yield toks\n",
    "\n",
    "def count_unigrams_bigrams(sents: Iterable[List[str]]) -> Tuple[Counter, Counter]:\n",
    "    \"\"\"\n",
    "    Accumulates unigram and bigram counts from an iterable of token lists.\n",
    "    \"\"\"\n",
    "    uni = Counter()\n",
    "    bi = Counter()\n",
    "    for toks in sents:\n",
    "        uni.update(toks)\n",
    "        for i in range(1, len(toks)):\n",
    "            bi[(toks[i-1], toks[i])] += 1\n",
    "    return uni, bi\n",
    "\n",
    "def mle_unigram_probs(uni: Counter) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    UnsMoothed Unigram MLE: P(w) = count(w) / total_tokens\n",
    "    \"\"\"\n",
    "    total = sum(uni.values())\n",
    "    return {w: c / total for w, c in uni.items()}\n",
    "\n",
    "def mle_bigram_probs(uni: Counter, bi: Counter) -> Dict[Tuple[str, str], float]:\n",
    "    \"\"\"\n",
    "    UnsMoothed Bigram MLE: P(w2 | w1) = count(w1, w2) / count(w1)\n",
    "    Only defined for observed bigrams.\n",
    "    \"\"\"\n",
    "    probs: Dict[Tuple[str, str], float] = {}\n",
    "    for (w1, w2), c12 in bi.items():\n",
    "        denom = uni[w1]\n",
    "        if denom > 0:\n",
    "            probs[(w1, w2)] = c12 / denom\n",
    "    return probs\n",
    "\n",
    "def save_tsv_unigram(path: str, uni_counts: Counter, uni_probs: Dict[str, float]) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"token\\tcount\\tprob\\n\")\n",
    "        for w, c in uni_counts.most_common():\n",
    "            f.write(f\"{w}\\t{c}\\t{uni_probs.get(w, 0.0):.12g}\\n\")\n",
    "\n",
    "def save_tsv_bigram(path: str, bi_counts: Counter, bi_probs: Dict[Tuple[str,str], float]) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"w1\\tw2\\tcount\\tprob\\n\")\n",
    "        for (w1, w2), c in sorted(bi_counts.items(), key=lambda kv: (-kv[1], kv[0][0], kv[0][1])):\n",
    "            f.write(f\"{w1}\\t{w2}\\t{c}\\t{bi_probs.get((w1,w2), 0.0):.12g}\\n\")\n",
    "\n",
    "def save_json(path: str, obj) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcd568c-a417-4ef9-a785-4cd14fe01aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure paths & options ---\n",
    "TRAIN_PATH = \"train.txt\"\n",
    "OUT_DIR = \"/outputs\"\n",
    "ADD_BOUNDARIES = True               # Wrap each line with <s> ... </s> for better bigrams\n",
    "TOPK = 30                           # How many top items to preview\n",
    "\n",
    "# --- Run pipeline ---\n",
    "sents = read_corpus(TRAIN_PATH, add_boundaries=ADD_BOUNDARIES)\n",
    "uni_counts, bi_counts = count_unigrams_bigrams(sents)\n",
    "uni_probs = mle_unigram_probs(uni_counts)\n",
    "bi_probs  = mle_bigram_probs(uni_counts, bi_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cd0141-d121-4384-9e3d-899b56e38ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic stats ---\n",
    "num_types = len(uni_counts)\n",
    "num_tokens = sum(uni_counts.values())\n",
    "num_bigram_types = len(bi_counts)\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    \"metric\": [\"#unigram_types\", \"#tokens\", \"#bigram_types\", \"add_boundaries\"],\n",
    "    \"value\":  [num_types, num_tokens, num_bigram_types, ADD_BOUNDARIES]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa70a9bf-cf3d-47c5-b385-f986a7db65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preview top-K ---\n",
    "df_uni = pd.DataFrame(uni_counts.most_common(TOPK), columns=[\"token\", \"count\"])\n",
    "df_uni[\"prob\"] = df_uni[\"token\"].apply(lambda w: uni_probs[w])\n",
    "\n",
    "df_bi = pd.DataFrame(bi_counts.most_common(TOPK), columns=[\"bigram\", \"count\"])\n",
    "df_bi[[\"w1\",\"w2\"]] = pd.DataFrame(df_bi[\"bigram\"].tolist(), index=df_bi.index)\n",
    "df_bi[\"prob\"] = df_bi.apply(lambda r: bi_probs.get((r[\"w1\"], r[\"w2\"]), 0.0), axis=1)\n",
    "df_bi = df_bi[[\"w1\",\"w2\",\"count\",\"prob\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358f2654-6cf8-4a3e-b972-6cdb19942b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-gram Stats ===\n",
      "           metric  value\n",
      "0  #unigram_types   7258\n",
      "1         #tokens  90708\n",
      "2   #bigram_types  40023\n",
      "3  add_boundaries   True\n",
      "\n",
      "=== Top Unigrams ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>4692</td>\n",
       "      <td>0.051726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>4250</td>\n",
       "      <td>0.046854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>2949</td>\n",
       "      <td>0.032511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>2552</td>\n",
       "      <td>0.028134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>2215</td>\n",
       "      <td>0.024419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>2077</td>\n",
       "      <td>0.022898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.020064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.018289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>1212</td>\n",
       "      <td>0.013362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.011465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The</td>\n",
       "      <td>1035</td>\n",
       "      <td>0.011410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>997</td>\n",
       "      <td>0.010991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hotel</td>\n",
       "      <td>923</td>\n",
       "      <td>0.010176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>room</td>\n",
       "      <td>877</td>\n",
       "      <td>0.009668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>is</td>\n",
       "      <td>835</td>\n",
       "      <td>0.009205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>at</td>\n",
       "      <td>717</td>\n",
       "      <td>0.007904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it</td>\n",
       "      <td>700</td>\n",
       "      <td>0.007717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>that</td>\n",
       "      <td>662</td>\n",
       "      <td>0.007298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>we</td>\n",
       "      <td>655</td>\n",
       "      <td>0.007221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>on</td>\n",
       "      <td>610</td>\n",
       "      <td>0.006725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>with</td>\n",
       "      <td>589</td>\n",
       "      <td>0.006493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>were</td>\n",
       "      <td>578</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>not</td>\n",
       "      <td>570</td>\n",
       "      <td>0.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>had</td>\n",
       "      <td>525</td>\n",
       "      <td>0.005788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>!</td>\n",
       "      <td>520</td>\n",
       "      <td>0.005733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>512</td>\n",
       "      <td>0.005644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>512</td>\n",
       "      <td>0.005644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>We</td>\n",
       "      <td>461</td>\n",
       "      <td>0.005082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>this</td>\n",
       "      <td>453</td>\n",
       "      <td>0.004994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>but</td>\n",
       "      <td>451</td>\n",
       "      <td>0.004972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token  count      prob\n",
       "0       .   4692  0.051726\n",
       "1     the   4250  0.046854\n",
       "2       ,   2949  0.032511\n",
       "3     and   2552  0.028134\n",
       "4       a   2215  0.024419\n",
       "5      to   2077  0.022898\n",
       "6     was   1820  0.020064\n",
       "7       I   1659  0.018289\n",
       "8      in   1212  0.013362\n",
       "9      of   1040  0.011465\n",
       "10    The   1035  0.011410\n",
       "11    for    997  0.010991\n",
       "12  hotel    923  0.010176\n",
       "13   room    877  0.009668\n",
       "14     is    835  0.009205\n",
       "15     at    717  0.007904\n",
       "16     it    700  0.007717\n",
       "17   that    662  0.007298\n",
       "18     we    655  0.007221\n",
       "19     on    610  0.006725\n",
       "20   with    589  0.006493\n",
       "21   were    578  0.006372\n",
       "22    not    570  0.006284\n",
       "23    had    525  0.005788\n",
       "24      !    520  0.005733\n",
       "25    <s>    512  0.005644\n",
       "26   </s>    512  0.005644\n",
       "27     We    461  0.005082\n",
       "28   this    453  0.004994\n",
       "29    but    451  0.004972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top Bigrams ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>The</td>\n",
       "      <td>858</td>\n",
       "      <td>0.182864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>I</td>\n",
       "      <td>522</td>\n",
       "      <td>0.111253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>388</td>\n",
       "      <td>0.320132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>371</td>\n",
       "      <td>0.079071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>342</td>\n",
       "      <td>0.328846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>We</td>\n",
       "      <td>339</td>\n",
       "      <td>0.072251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>322</td>\n",
       "      <td>0.109190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>321</td>\n",
       "      <td>0.447699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>hotel</td>\n",
       "      <td>287</td>\n",
       "      <td>0.067529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>268</td>\n",
       "      <td>0.105016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>263</td>\n",
       "      <td>0.126625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,</td>\n",
       "      <td>but</td>\n",
       "      <td>243</td>\n",
       "      <td>0.082401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>,</td>\n",
       "      <td>the</td>\n",
       "      <td>242</td>\n",
       "      <td>0.082062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>on</td>\n",
       "      <td>the</td>\n",
       "      <td>213</td>\n",
       "      <td>0.349180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>room</td>\n",
       "      <td>197</td>\n",
       "      <td>0.046353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I</td>\n",
       "      <td>was</td>\n",
       "      <td>183</td>\n",
       "      <td>0.110307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>a</td>\n",
       "      <td>177</td>\n",
       "      <td>0.177533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>room</td>\n",
       "      <td>was</td>\n",
       "      <td>177</td>\n",
       "      <td>0.201824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hotel</td>\n",
       "      <td>.</td>\n",
       "      <td>173</td>\n",
       "      <td>0.187432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>.</td>\n",
       "      <td>It</td>\n",
       "      <td>172</td>\n",
       "      <td>0.036658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>this</td>\n",
       "      <td>hotel</td>\n",
       "      <td>168</td>\n",
       "      <td>0.370861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,</td>\n",
       "      <td>I</td>\n",
       "      <td>167</td>\n",
       "      <td>0.056629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>it</td>\n",
       "      <td>was</td>\n",
       "      <td>151</td>\n",
       "      <td>0.215714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>147</td>\n",
       "      <td>0.282692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>146</td>\n",
       "      <td>0.146439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>was</td>\n",
       "      <td>very</td>\n",
       "      <td>145</td>\n",
       "      <td>0.079670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>137</td>\n",
       "      <td>0.393678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>and</td>\n",
       "      <td>I</td>\n",
       "      <td>134</td>\n",
       "      <td>0.052508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>was</td>\n",
       "      <td>a</td>\n",
       "      <td>133</td>\n",
       "      <td>0.073077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>129</td>\n",
       "      <td>0.245714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w1     w2  count      prob\n",
       "0       .    The    858  0.182864\n",
       "1       .      I    522  0.111253\n",
       "2      in    the    388  0.320132\n",
       "3       .   </s>    371  0.079071\n",
       "4      of    the    342  0.328846\n",
       "5       .     We    339  0.072251\n",
       "6       ,    and    322  0.109190\n",
       "7      at    the    321  0.447699\n",
       "8     the  hotel    287  0.067529\n",
       "9     and    the    268  0.105016\n",
       "10     to    the    263  0.126625\n",
       "11      ,    but    243  0.082401\n",
       "12      ,    the    242  0.082062\n",
       "13     on    the    213  0.349180\n",
       "14    the   room    197  0.046353\n",
       "15      I    was    183  0.110307\n",
       "16    for      a    177  0.177533\n",
       "17   room    was    177  0.201824\n",
       "18  hotel      .    173  0.187432\n",
       "19      .     It    172  0.036658\n",
       "20   this  hotel    168  0.370861\n",
       "21      ,      I    167  0.056629\n",
       "22     it    was    151  0.215714\n",
       "23      !      !    147  0.282692\n",
       "24    for    the    146  0.146439\n",
       "25    was   very    145  0.079670\n",
       "26   from    the    137  0.393678\n",
       "27    and      I    134  0.052508\n",
       "28    was      a    133  0.073077\n",
       "29    had      a    129  0.245714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Show results ---\n",
    "print(\"=== N-gram Stats ===\")\n",
    "print(stats)\n",
    "\n",
    "print(\"\\n=== Top Unigrams ===\")\n",
    "display(df_uni)\n",
    "\n",
    "print(\"\\n=== Top Bigrams ===\")\n",
    "display(df_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9a2cb4-6401-42cf-8a4c-40b3c7818d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " - Unigram counts (JSON): /outputs\\unigram_counts.json\n",
      " - Bigram counts   (JSON): /outputs\\bigram_counts.json\n",
      " - Unigram probs   (JSON): /outputs\\unigram_probs.json\n",
      " - Bigram probs    (JSON): /outputs\\bigram_probs.json\n",
      " - Unigram MLE (TSV):      /outputs\\unigram_mle.tsv\n",
      " - Bigram MLE  (TSV):      /outputs\\bigram_mle.tsv\n"
     ]
    }
   ],
   "source": [
    "# --- Save outputs ---\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "save_tsv_unigram(os.path.join(OUT_DIR, \"unigram_mle.tsv\"), uni_counts, uni_probs)\n",
    "save_tsv_bigram (os.path.join(OUT_DIR, \"bigram_mle.tsv\"),  bi_counts,  bi_probs)\n",
    "save_json(os.path.join(OUT_DIR, \"unigram_counts.json\"), {w: int(c) for w, c in uni_counts.items()})\n",
    "save_json(os.path.join(OUT_DIR, \"bigram_counts.json\"),  {\" || \".join([w1, w2]): int(c) for (w1, w2), c in bi_counts.items()})\n",
    "save_json(os.path.join(OUT_DIR, \"unigram_probs.json\"),  uni_probs)\n",
    "save_json(os.path.join(OUT_DIR, \"bigram_probs.json\"),   {\" || \".join([w1, w2]): p for (w1, w2), p in bi_probs.items()})\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(\" - Unigram counts (JSON):\", os.path.join(OUT_DIR, \"unigram_counts.json\"))\n",
    "print(\" - Bigram counts   (JSON):\", os.path.join(OUT_DIR, \"bigram_counts.json\"))\n",
    "print(\" - Unigram probs   (JSON):\", os.path.join(OUT_DIR, \"unigram_probs.json\"))\n",
    "print(\" - Bigram probs    (JSON):\", os.path.join(OUT_DIR, \"bigram_probs.json\"))\n",
    "print(\" - Unigram MLE (TSV):     \", os.path.join(OUT_DIR, \"unigram_mle.tsv\"))\n",
    "print(\" - Bigram MLE  (TSV):     \", os.path.join(OUT_DIR, \"bigram_mle.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6df733-1a2f-411e-a1b4-fc8f13eb3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP A1 â€” Section 4: Unknown-word handling + Smoothing (Jupyter-ready)\n",
    "# -------------------------------------------------------------------\n",
    "# This cell extends the previous unsmoothed n-gram code with:\n",
    "# 1) Unknown-word handling via <unk> using a frequency threshold.\n",
    "# 2) Two smoothing methods for probabilities:\n",
    "#    (A) Add-k / Laplace smoothing (k configurable; k=1 is Laplace).\n",
    "#    (B) Jelinekâ€“Mercer (JM) interpolation for bigrams: \n",
    "#        P_JM(w2|w1) = Î» * P_MLE_bigram(w2|w1) + (1-Î») * P_unigram(w2)\n",
    "#\n",
    "# Design:\n",
    "# - Build vocabulary from training counts with a min_freq threshold.\n",
    "# - Map all tokens not in vocab to <unk>, then re-count unigrams/bigrams.\n",
    "# - Provide probability functions for each smoothing strategy.\n",
    "#\n",
    "# This prepares everything you need for Section 5 (perplexity on validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189c2d37-9db8-4b29-b557-53111bf63b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utilities (from before)\n",
    "# -----------------------\n",
    "# def read_corpus(path: str, add_boundaries: bool = True) -> List[List[str]]:\n",
    "#     sents: List[List[str]] = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             toks = line.split()\n",
    "#             if add_boundaries:\n",
    "#                 toks = [BOS] + toks + [EOS]\n",
    "#             sents.append(toks)\n",
    "#     return sents\n",
    "\n",
    "def read_corpus(path: str, add_boundaries: bool = False) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Reads a tokenized corpus; each line is a review with space-delimited tokens.\n",
    "    If add_boundaries=True, wraps each line with BOS/EOS.\n",
    "    Returns: List[List[str]] where each inner list is the token sequence for one line.\n",
    "    \"\"\"\n",
    "    sents: List[List[str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            toks = trans_lower(toks)\n",
    "            if add_boundaries:\n",
    "                toks = [BOS] + toks + [EOS]\n",
    "            sents.append(toks)\n",
    "    return sents\n",
    "    \n",
    "def count_unigrams_bigrams(sents: Iterable[List[str]]) -> Tuple[Counter, Counter]:\n",
    "    uni = Counter()\n",
    "    bi = Counter()\n",
    "    for toks in sents:\n",
    "        uni.update(toks)\n",
    "        for i in range(1, len(toks)):\n",
    "            bi[(toks[i-1], toks[i])] += 1\n",
    "    return uni, bi\n",
    "\n",
    "# -----------------------\n",
    "# Unknown-word handling\n",
    "# -----------------------\n",
    "def build_vocab_from_counts(uni_counts: Counter, min_freq: int = 1,\n",
    "                            include_special: bool = True) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Returns a vocabulary set using a frequency threshold.\n",
    "    All tokens with count < min_freq will be mapped to <unk> later.\n",
    "    \"\"\"\n",
    "    vocab = {w for w, c in uni_counts.items() if c >= min_freq}\n",
    "    if include_special:\n",
    "        vocab |= {BOS, EOS, UNK}\n",
    "    return vocab\n",
    "\n",
    "def map_tokens_to_vocab(sents: Iterable[List[str]], vocab: Set[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Replace OOV tokens with <unk> given a fixed vocabulary.\n",
    "    \"\"\"\n",
    "    mapped: List[List[str]] = []\n",
    "    for toks in sents:\n",
    "        mapped.append([t if t in vocab else UNK for t in toks])\n",
    "    return mapped\n",
    "\n",
    "# -----------------------\n",
    "# Smoothing A: Add-k\n",
    "# -----------------------\n",
    "def add_k_unigram_prob(w: str, uni_counts: Counter, V: int, k: float) -> float:\n",
    "    N = sum(uni_counts.values())\n",
    "    return (uni_counts[w] + k) / (N + k * V)\n",
    "\n",
    "def add_k_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter,\n",
    "                      V: int, k: float) -> float:\n",
    "    # denominator: count(w1) smoothed with k*V\n",
    "    return (bi_counts[(w1, w2)] + k) / (uni_counts[w1] + k * V)\n",
    "\n",
    "# -----------------------\n",
    "# Smoothing B: Jelinekâ€“Mercer\n",
    "# -----------------------\n",
    "def mle_unigram_prob(w: str, uni_counts: Counter) -> float:\n",
    "    N = sum(uni_counts.values())\n",
    "    return uni_counts[w] / N if N > 0 else 0.0\n",
    "\n",
    "def mle_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter) -> float:\n",
    "    denom = uni_counts[w1]\n",
    "    return (bi_counts[(w1, w2)] / denom) if denom > 0 else 0.0\n",
    "\n",
    "def jm_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter,\n",
    "                   lamb: float, # 0 <= lamb <= 1\n",
    "                   backoff_unigram: str = \"addk\",\n",
    "                   addk_k: float = 1.0,\n",
    "                   V: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Jelinekâ€“Mercer interpolation:\n",
    "      P_JM(w2|w1) = Î» * P_MLE_bigram(w2|w1) + (1-Î») * P_unigram(w2)\n",
    "    For the unigram component, you can use MLE or add-k; default uses add-k for robustness.\n",
    "    \"\"\"\n",
    "    p_bg = mle_bigram_prob(w1, w2, uni_counts, bi_counts)\n",
    "    if backoff_unigram == \"mle\":\n",
    "        p_uni = mle_unigram_prob(w2, uni_counts)\n",
    "    else:\n",
    "        assert V is not None, \"V (vocab size) required for add-k unigram.\"\n",
    "        p_uni = add_k_unigram_prob(w2, uni_counts, V=V, k=addk_k)\n",
    "    return lamb * p_bg + (1.0 - lamb) * p_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eecc598-ebd3-4231-8c95-e0fa8a17ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>min_freq</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V (vocab size)</td>\n",
       "      <td>3412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#tokens (N)</td>\n",
       "      <td>90708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#bigram types</td>\n",
       "      <td>34160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric  value\n",
       "0        min_freq      2\n",
       "1  V (vocab size)   3412\n",
       "2     #tokens (N)  90708\n",
       "3   #bigram types  34160"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Train with UNK mapping\n",
    "# -----------------------\n",
    "TRAIN_PATH = \"train.txt\"\n",
    "ADD_BOUNDARIES = True\n",
    "MIN_FREQ = 2   # tokens with count < MIN_FREQ become <unk>\n",
    "\n",
    "# 1) Raw counts\n",
    "raw_sents = read_corpus(TRAIN_PATH, add_boundaries=ADD_BOUNDARIES)\n",
    "raw_uni, raw_bi = count_unigrams_bigrams(raw_sents)\n",
    "\n",
    "# 2) Build vocab & map to <unk>\n",
    "vocab = build_vocab_from_counts(raw_uni, min_freq=MIN_FREQ, include_special=True)\n",
    "mapped_sents = map_tokens_to_vocab(raw_sents, vocab)\n",
    "\n",
    "# 3) Re-count after mapping (this is the model's actual counts)\n",
    "uni_counts, bi_counts = count_unigrams_bigrams(mapped_sents)\n",
    "V = len(vocab)  # vocabulary size INCLUDING <unk>, <s>, </s>\n",
    "\n",
    "# Preview vocabulary stats\n",
    "stats = pd.DataFrame({\n",
    "    \"metric\": [\"min_freq\", \"V (vocab size)\", \"#tokens (N)\", \"#bigram types\"],\n",
    "    \"value\":  [MIN_FREQ, V, sum(uni_counts.values()), len(bi_counts)]\n",
    "})\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7da94f2-1df3-4430-ad0b-60940bbe2f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>P_addk_k1</th>\n",
       "      <th>P_addk_k01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>4692</td>\n",
       "      <td>0.049862</td>\n",
       "      <td>0.051534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>4250</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>0.046679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>3847</td>\n",
       "      <td>0.040884</td>\n",
       "      <td>0.042253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>2949</td>\n",
       "      <td>0.031343</td>\n",
       "      <td>0.032390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>2552</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>0.028030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>2215</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.024329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>2077</td>\n",
       "      <td>0.022078</td>\n",
       "      <td>0.022813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>was</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.019348</td>\n",
       "      <td>0.019990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.017637</td>\n",
       "      <td>0.018222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>1212</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.013313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.011423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The</td>\n",
       "      <td>1035</td>\n",
       "      <td>0.011007</td>\n",
       "      <td>0.011369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>997</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.010951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hotel</td>\n",
       "      <td>923</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.010138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>room</td>\n",
       "      <td>877</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.009633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>835</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.009172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>at</td>\n",
       "      <td>717</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>0.007876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>it</td>\n",
       "      <td>700</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.007689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>that</td>\n",
       "      <td>662</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>0.007272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>we</td>\n",
       "      <td>655</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.007195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token  count  P_addk_k1  P_addk_k01\n",
       "0       .   4692   0.049862    0.051534\n",
       "1     the   4250   0.045166    0.046679\n",
       "2   <unk>   3847   0.040884    0.042253\n",
       "3       ,   2949   0.031343    0.032390\n",
       "4     and   2552   0.027125    0.028030\n",
       "5       a   2215   0.023544    0.024329\n",
       "6      to   2077   0.022078    0.022813\n",
       "7     was   1820   0.019348    0.019990\n",
       "8       I   1659   0.017637    0.018222\n",
       "9      in   1212   0.012888    0.013313\n",
       "10     of   1040   0.011060    0.011423\n",
       "11    The   1035   0.011007    0.011369\n",
       "12    for    997   0.010603    0.010951\n",
       "13  hotel    923   0.009817    0.010138\n",
       "14   room    877   0.009329    0.009633\n",
       "15     is    835   0.008882    0.009172\n",
       "16     at    717   0.007629    0.007876\n",
       "17     it    700   0.007448    0.007689\n",
       "18   that    662   0.007044    0.007272\n",
       "19     we    655   0.006970    0.007195"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>count</th>\n",
       "      <th>P_addk_k1</th>\n",
       "      <th>P_addk_k01</th>\n",
       "      <th>P_JM_l0.7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>The</td>\n",
       "      <td>858</td>\n",
       "      <td>0.105997</td>\n",
       "      <td>0.170488</td>\n",
       "      <td>0.131416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>I</td>\n",
       "      <td>522</td>\n",
       "      <td>0.064536</td>\n",
       "      <td>0.103731</td>\n",
       "      <td>0.083344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>.</td>\n",
       "      <td>418</td>\n",
       "      <td>0.057721</td>\n",
       "      <td>0.099828</td>\n",
       "      <td>0.091519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>388</td>\n",
       "      <td>0.084126</td>\n",
       "      <td>0.249871</td>\n",
       "      <td>0.238096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>371</td>\n",
       "      <td>0.045903</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.057037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>342</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>0.247683</td>\n",
       "      <td>0.244196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>We</td>\n",
       "      <td>339</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.067373</td>\n",
       "      <td>0.052095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>322</td>\n",
       "      <td>0.050778</td>\n",
       "      <td>0.097897</td>\n",
       "      <td>0.084842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>321</td>\n",
       "      <td>0.077985</td>\n",
       "      <td>0.303440</td>\n",
       "      <td>0.327393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>hotel</td>\n",
       "      <td>287</td>\n",
       "      <td>0.037588</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.050312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>286</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>0.068311</td>\n",
       "      <td>0.061758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>280</td>\n",
       "      <td>0.038711</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>0.063625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>274</td>\n",
       "      <td>0.035891</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.057805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>the</td>\n",
       "      <td>268</td>\n",
       "      <td>0.045104</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.087515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>263</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.102641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>248</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>0.049293</td>\n",
       "      <td>0.049675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>,</td>\n",
       "      <td>but</td>\n",
       "      <td>243</td>\n",
       "      <td>0.038359</td>\n",
       "      <td>0.073886</td>\n",
       "      <td>0.059167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,</td>\n",
       "      <td>the</td>\n",
       "      <td>242</td>\n",
       "      <td>0.038202</td>\n",
       "      <td>0.073582</td>\n",
       "      <td>0.071447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>219</td>\n",
       "      <td>0.039097</td>\n",
       "      <td>0.085713</td>\n",
       "      <td>0.081886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>on</td>\n",
       "      <td>the</td>\n",
       "      <td>213</td>\n",
       "      <td>0.053207</td>\n",
       "      <td>0.224033</td>\n",
       "      <td>0.258430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       w1     w2  count  P_addk_k1  P_addk_k01  P_JM_l0.7\n",
       "0       .    The    858   0.105997    0.170488   0.131416\n",
       "1       .      I    522   0.064536    0.103731   0.083344\n",
       "2   <unk>      .    418   0.057721    0.099828   0.091519\n",
       "3      in    the    388   0.084126    0.249871   0.238096\n",
       "4       .   </s>    371   0.045903    0.073730   0.057037\n",
       "5      of    the    342   0.077044    0.247683   0.244196\n",
       "6       .     We    339   0.041955    0.067373   0.052095\n",
       "7       ,    and    322   0.050778    0.097897   0.084842\n",
       "8      at    the    321   0.077985    0.303440   0.327393\n",
       "9     the  hotel    287   0.037588    0.062533   0.050312\n",
       "10  <unk>      ,    286   0.039537    0.068311   0.061758\n",
       "11  <unk>  <unk>    280   0.038711    0.066878   0.063625\n",
       "12    the  <unk>    274   0.035891    0.059701   0.057805\n",
       "13    and    the    268   0.045104    0.092666   0.087515\n",
       "14     to    the    263   0.048096    0.108800   0.102641\n",
       "15      .  <unk>    248   0.030726    0.049293   0.049675\n",
       "16      ,    but    243   0.038359    0.073886   0.059167\n",
       "17      ,    the    242   0.038202    0.073582   0.071447\n",
       "18      a  <unk>    219   0.039097    0.085713   0.081886\n",
       "19     on    the    213   0.053207    0.224033   0.258430"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Example: compute probs with each smoother\n",
    "# -----------------------\n",
    "TOPK = 20\n",
    "df_uni = pd.DataFrame(uni_counts.most_common(TOPK), columns=[\"token\", \"count\"])\n",
    "# Add-k unigram (k=1 Laplace; try also k=0.1)\n",
    "df_uni[\"P_addk_k1\"]  = df_uni[\"token\"].apply(lambda w: add_k_unigram_prob(w, uni_counts, V=V, k=1.0))\n",
    "df_uni[\"P_addk_k01\"] = df_uni[\"token\"].apply(lambda w: add_k_unigram_prob(w, uni_counts, V=V, k=0.1))\n",
    "display(df_uni)\n",
    "\n",
    "# For bigrams, show top by count and compute smoothed probs\n",
    "df_bi = pd.DataFrame(bi_counts.most_common(TOPK), columns=[\"bigram\", \"count\"])\n",
    "df_bi[[\"w1\",\"w2\"]] = pd.DataFrame(df_bi[\"bigram\"].tolist(), index=df_bi.index)\n",
    "\n",
    "# Add-k bigram (k=1 and k=0.1)\n",
    "df_bi[\"P_addk_k1\"]  = df_bi.apply(lambda r: add_k_bigram_prob(r[\"w1\"], r[\"w2\"], uni_counts, bi_counts, V=V, k=1.0), axis=1)\n",
    "df_bi[\"P_addk_k01\"] = df_bi.apply(lambda r: add_k_bigram_prob(r[\"w1\"], r[\"w2\"], uni_counts, bi_counts, V=V, k=0.1), axis=1)\n",
    "\n",
    "# Jelinekâ€“Mercer with Î»=0.7 (tune later)\n",
    "LAMB = 0.7\n",
    "df_bi[\"P_JM_l0.7\"] = df_bi.apply(lambda r: jm_bigram_prob(r[\"w1\"], r[\"w2\"], uni_counts, bi_counts,\n",
    "                                                          lamb=LAMB, backoff_unigram=\"addk\", addk_k=0.1, V=V), axis=1)\n",
    "display(df_bi[[\"w1\",\"w2\",\"count\",\"P_addk_k1\",\"P_addk_k01\",\"P_JM_l0.7\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86018686-1c86-44d4-97cb-cecae131a421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved in: /outputs\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Save artifacts for report/reuse\n",
    "# -----------------------\n",
    "OUT_DIR = \"/outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save vocabulary\n",
    "with open(os.path.join(OUT_DIR, \"vocab.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in sorted(vocab):\n",
    "        f.write(w + \"\\n\")\n",
    "\n",
    "# Save counts\n",
    "with open(os.path.join(OUT_DIR, \"unigram_counts.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({w: int(c) for w, c in uni_counts.items()}, f, ensure_ascii=False)\n",
    "with open(os.path.join(OUT_DIR, \"bigram_counts.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\" || \".join([w1, w2]): int(c) for (w1, w2), c in bi_counts.items()}, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Artifacts saved in:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34cdfac0-8bea-4c96-9738-2c49a302dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP A1 â€” Section 5: Perplexity on Validation Set (Jupyter-ready)\n",
    "# -----------------------------------------------------------------\n",
    "# This cell computes perplexity on /mnt/data/val.txt for multiple models:\n",
    "#   - Unigram Add-k (k in {1.0, 0.1})\n",
    "#   - Bigram Add-k (k in {1.0, 0.1})\n",
    "#   - Bigram Jelinekâ€“Mercer (Î» in {0.7, 0.5})\n",
    "#\n",
    "# It REBUILDS the training vocab and counts (with UNK mapping) to be self-contained.\n",
    "# Validation tokens are mapped to <unk> using the TRAIN vocabulary.\n",
    "#\n",
    "# PP = exp( (1/N) * sum_i [ -log P(w_i | history) ] )\n",
    "# We use natural log; PP uses exp of mean negative log-probability.\n",
    "#\n",
    "# You can tweak: MIN_FREQ, ADD_BOUNDARIES, K values, and LAMBDA values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dcbd560-727a-4f82-a808-934406d41e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Iterable, Set\n",
    "import os, json, math\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d2da87-7fa7-41b1-af13-42e4f9bb965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Constants / Settings\n",
    "# -----------------------\n",
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "TRAIN_PATH = \"train.txt\"\n",
    "VAL_PATH   = \"val.txt\"\n",
    "ADD_BOUNDARIES = True\n",
    "MIN_FREQ = 2\n",
    "\n",
    "ADDK_GRID_UNI = [1.0, 0.1]\n",
    "ADDK_GRID_BI  = [1.0, 0.1]\n",
    "JM_LAMBDAS    = [0.7, 0.5]\n",
    "JM_UNIGRAM_BACKOFF = \"addk\"   # \"addk\" or \"mle\"\n",
    "JM_ADDK_K = 0.1               # used if backoff == \"addk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4b9ec7-1781-4209-a030-5eb37a6ac046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def read_corpus(path: str, add_boundaries: bool = True) -> List[List[str]]:\n",
    "    sents: List[List[str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split()\n",
    "            if add_boundaries:\n",
    "                toks = [BOS] + toks + [EOS]\n",
    "            sents.append(toks)\n",
    "    return sents\n",
    "\n",
    "# def read_corpus(path: str, add_boundaries: bool = False) -> List[List[str]]:\n",
    "#     \"\"\"\n",
    "#     Reads a tokenized corpus; each line is a review with space-delimited tokens.\n",
    "#     If add_boundaries=True, wraps each line with BOS/EOS.\n",
    "#     Returns: List[List[str]] where each inner list is the token sequence for one line.\n",
    "#     \"\"\"\n",
    "#     sents: List[List[str]] = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             toks = line.split()\n",
    "#             toks = trans_lower(toks)\n",
    "#             if add_boundaries:\n",
    "#                 toks = [BOS] + toks + [EOS]\n",
    "#             sents.append(toks)\n",
    "#     return sents\n",
    "\n",
    "def count_unigrams_bigrams(sents: Iterable[List[str]]) -> Tuple[Counter, Counter]:\n",
    "    uni = Counter()\n",
    "    bi = Counter()\n",
    "    for toks in sents:\n",
    "        uni.update(toks)\n",
    "        for i in range(1, len(toks)):\n",
    "            bi[(toks[i-1], toks[i])] += 1\n",
    "    return uni, bi\n",
    "\n",
    "def build_vocab_from_counts(uni_counts: Counter, min_freq: int = 1,\n",
    "                            include_special: bool = True) -> Set[str]:\n",
    "    vocab = {w for w, c in uni_counts.items() if c >= min_freq}\n",
    "    if include_special:\n",
    "        vocab |= {BOS, EOS, UNK}\n",
    "    return vocab\n",
    "\n",
    "def map_tokens_to_vocab(sents: Iterable[List[str]], vocab: Set[str]) -> List[List[str]]:\n",
    "    return [[t if t in vocab else UNK for t in toks] for toks in sents]\n",
    "\n",
    "# Prob functions\n",
    "def add_k_unigram_prob(w: str, uni_counts: Counter, V: int, k: float) -> float:\n",
    "    N = sum(uni_counts.values())\n",
    "    return (uni_counts[w] + k) / (N + k * V)\n",
    "\n",
    "def add_k_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter,\n",
    "                      V: int, k: float) -> float:\n",
    "    return (bi_counts[(w1, w2)] + k) / (uni_counts[w1] + k * V)\n",
    "\n",
    "def mle_unigram_prob(w: str, uni_counts: Counter) -> float:\n",
    "    N = sum(uni_counts.values())\n",
    "    return uni_counts[w] / N if N > 0 else 0.0\n",
    "\n",
    "def mle_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter) -> float:\n",
    "    denom = uni_counts[w1]\n",
    "    return (bi_counts[(w1, w2)] / denom) if denom > 0 else 0.0\n",
    "\n",
    "def jm_bigram_prob(w1: str, w2: str, uni_counts: Counter, bi_counts: Counter,\n",
    "                   lamb: float, backoff_unigram: str = \"addk\",\n",
    "                   addk_k: float = 0.1, V: int = None) -> float:\n",
    "    p_bg = mle_bigram_prob(w1, w2, uni_counts, bi_counts)\n",
    "    if backoff_unigram == \"mle\":\n",
    "        p_uni = mle_unigram_prob(w2, uni_counts)\n",
    "    else:\n",
    "        assert V is not None\n",
    "        p_uni = add_k_unigram_prob(w2, uni_counts, V=V, k=addk_k)\n",
    "    return lamb * p_bg + (1.0 - lamb) * p_uni\n",
    "\n",
    "# Perplexity\n",
    "def corpus_perplexity_unigram(sents: Iterable[List[str]], prob_fn) -> float:\n",
    "    logprob = 0.0\n",
    "    N = 0\n",
    "    for toks in sents:\n",
    "        for w in toks:\n",
    "            p = prob_fn(w)\n",
    "            if p <= 0.0:\n",
    "                return float(\"inf\")\n",
    "            logprob += math.log(p)\n",
    "            N += 1\n",
    "    return math.exp(-logprob / max(N, 1))\n",
    "\n",
    "def corpus_perplexity_bigram(sents: Iterable[List[str]], prob_fn) -> float:\n",
    "    logprob = 0.0\n",
    "    N = 0\n",
    "    for toks in sents:\n",
    "        for i in range(1, len(toks)):\n",
    "            w1, w2 = toks[i-1], toks[i]\n",
    "            p = prob_fn(w1, w2)\n",
    "            if p <= 0.0:\n",
    "                return float(\"inf\")\n",
    "            logprob += math.log(p)\n",
    "            N += 1\n",
    "    return math.exp(-logprob / max(N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d336161-6cc3-4931-8c81-a53eb5085eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train vocab & counts\n",
    "# -----------------------\n",
    "train_raw = read_corpus(TRAIN_PATH, add_boundaries=ADD_BOUNDARIES)\n",
    "train_uni_raw, _ = count_unigrams_bigrams(train_raw)\n",
    "vocab = build_vocab_from_counts(train_uni_raw, min_freq=MIN_FREQ, include_special=True)\n",
    "\n",
    "# Map both train and validation to vocab\n",
    "train = map_tokens_to_vocab(train_raw, vocab)\n",
    "val_raw = read_corpus(VAL_PATH, add_boundaries=ADD_BOUNDARIES)\n",
    "val = map_tokens_to_vocab(val_raw, vocab)\n",
    "\n",
    "# Re-count on mapped train (model counts)\n",
    "uni_counts, bi_counts = count_unigrams_bigrams(train)\n",
    "V = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbbac75-1333-43c6-a97a-ee828edbfc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>PP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigram-jm</td>\n",
       "      <td>lambda=0.7, backoff=addk, k=0.1</td>\n",
       "      <td>105.376360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bigram-jm</td>\n",
       "      <td>lambda=0.5, backoff=addk, k=0.1</td>\n",
       "      <td>110.984653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bigram-addk</td>\n",
       "      <td>k=0.1</td>\n",
       "      <td>193.872483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unigram-addk</td>\n",
       "      <td>k=0.1</td>\n",
       "      <td>313.099999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram-addk</td>\n",
       "      <td>k=1.0</td>\n",
       "      <td>315.863378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigram-addk</td>\n",
       "      <td>k=1.0</td>\n",
       "      <td>467.468485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model                           params          PP\n",
       "4     bigram-jm  lambda=0.7, backoff=addk, k=0.1  105.376360\n",
       "5     bigram-jm  lambda=0.5, backoff=addk, k=0.1  110.984653\n",
       "3   bigram-addk                            k=0.1  193.872483\n",
       "1  unigram-addk                            k=0.1  313.099999\n",
       "0  unigram-addk                            k=1.0  315.863378\n",
       "2   bigram-addk                            k=1.0  467.468485"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: /mnt/data/ngram_outputs_section5\\perplexity_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Evaluate multiple models\n",
    "# -----------------------\n",
    "records = []\n",
    "\n",
    "# Unigram Add-k\n",
    "for k in ADDK_GRID_UNI:\n",
    "    def _uni_pk(w, k=k):\n",
    "        return add_k_unigram_prob(w, uni_counts, V=V, k=k)\n",
    "    pp = corpus_perplexity_unigram(val, _uni_pk)\n",
    "    records.append({\"model\": \"unigram-addk\", \"params\": f\"k={k}\", \"PP\": pp})\n",
    "\n",
    "# Bigram Add-k\n",
    "for k in ADDK_GRID_BI:\n",
    "    def _bi_pk(w1, w2, k=k):\n",
    "        return add_k_bigram_prob(w1, w2, uni_counts, bi_counts, V=V, k=k)\n",
    "    pp = corpus_perplexity_bigram(val, _bi_pk)\n",
    "    records.append({\"model\": \"bigram-addk\", \"params\": f\"k={k}\", \"PP\": pp})\n",
    "\n",
    "# Bigram JM\n",
    "for lamb in JM_LAMBDAS:\n",
    "    def _bi_pjm(w1, w2, lamb=lamb):\n",
    "        return jm_bigram_prob(w1, w2, uni_counts, bi_counts, lamb=lamb,\n",
    "                              backoff_unigram=JM_UNIGRAM_BACKOFF, addk_k=JM_ADDK_K, V=V)\n",
    "    pp = corpus_perplexity_bigram(val, _bi_pjm)\n",
    "    records.append({\"model\": \"bigram-jm\", \"params\": f\"lambda={lamb}, backoff={JM_UNIGRAM_BACKOFF}, k={JM_ADDK_K}\", \"PP\": pp})\n",
    "\n",
    "results = pd.DataFrame(records).sort_values(\"PP\")\n",
    "display(results)\n",
    "\n",
    "# Save\n",
    "OUT_DIR = \"/mnt/data/ngram_outputs_section5\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "results.to_csv(os.path.join(OUT_DIR, \"perplexity_results.csv\"), index=False)\n",
    "with open(os.path.join(OUT_DIR, \"train_vocab.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for w in sorted(vocab):\n",
    "        f.write(w + \"\\n\")\n",
    "with open(os.path.join(OUT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"ADD_BOUNDARIES\": ADD_BOUNDARIES,\n",
    "        \"MIN_FREQ\": MIN_FREQ,\n",
    "        \"ADDK_GRID_UNI\": ADDK_GRID_UNI,\n",
    "        \"ADDK_GRID_BI\": ADDK_GRID_BI,\n",
    "        \"JM_LAMBDAS\": JM_LAMBDAS,\n",
    "        \"JM_UNIGRAM_BACKOFF\": JM_UNIGRAM_BACKOFF,\n",
    "        \"JM_ADDK_K\": JM_ADDK_K,\n",
    "        \"V\": V\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved results to:\", os.path.join(OUT_DIR, \"perplexity_results.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
